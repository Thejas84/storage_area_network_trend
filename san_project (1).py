# -*- coding: utf-8 -*-
"""SAN_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5lOouSsV0F0bmGG-vlnurIaDokftuAS
"""

pip install catboost tensorflow

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from catboost import CatBoostRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt


# Load synthetic data
data_path = 'https://raw.githubusercontent.com/Thejas84/SAN_ML/refs/heads/main/synthetic_san_dataset.csv'
data = pd.read_csv(data_path)


# Data Preprocessing
scaler = StandardScaler()
data['Diff'] = data.iloc[:, 1].diff().fillna(0)  # Calculate deviation (Diff)
data_normalized = scaler.fit_transform(data.iloc[:, 1:])



# Smoothing data with moving average (window=50)
data['Smoothed'] = data['Diff'].rolling(window=50).mean().fillna(0)



# Visualize raw and preprocessed data (Figures 1, 3-6)
plt.figure(figsize=(12, 6))
plt.plot(data.iloc[:, 1], label='Raw Data', alpha=0.7)
plt.title('Original Dataset (Figure 1)')
plt.xlabel('Sample Index')
plt.ylabel('SAN Load')
plt.legend()
plt.show()



plt.figure(figsize=(12, 6))
plt.plot(data['Diff'], label='Deviation (Diff)', alpha=0.7)
plt.plot(data['Smoothed'], label='Smoothed (Moving Avg)', alpha=0.7)
plt.title('Deviation and Smoothed Data (Figures 3-6)')
plt.xlabel('Sample Index')
plt.ylabel('SAN Load')
plt.legend()
plt.show()



# Generate Synthetic Data Visualizations (Figure 12)
synthetic_data_sample = data.iloc[:500]  # Show a portion of the synthetic data
plt.figure(figsize=(12, 6))
plt.plot(synthetic_data_sample.index, synthetic_data_sample.iloc[:, 1], label='Synthetic Data', alpha=0.7)
plt.title('Synthetic Data Example (Figure 12)')
plt.xlabel('Sample Index')
plt.ylabel('SAN Load')
plt.legend()
plt.show()



# Split into features and target
X = data_normalized[:, :-1]
y = data_normalized[:, -1]


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# CatBoost Feature Importance
catboost_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=0)
catboost_model.fit(X_train, y_train)
feature_importances = catboost_model.get_feature_importance()
important_features_indices = np.argsort(feature_importances)[-10:]
X_train_important = X_train[:, important_features_indices]
X_test_important = X_test[:, important_features_indices]


# Visualize Feature Importances (Additional Visualization)
plt.figure(figsize=(12, 6))
plt.barh(range(len(feature_importances)), feature_importances, align='center')
plt.title('Feature Importances (CatBoost)')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()


# Prepare LSTM Data
n_timesteps = 10


def create_lstm_data(X, y, timesteps):
    X_lstm, y_lstm = [], []
    for i in range(timesteps, len(X)):
        X_lstm.append(X[i-timesteps:i, :])
        y_lstm.append(y[i])
    return np.array(X_lstm), np.array(y_lstm)


X_train_lstm, y_train_lstm = create_lstm_data(X_train_important, y_train, n_timesteps)
X_test_lstm, y_test_lstm = create_lstm_data(X_test_important, y_test, n_timesteps)


# Define and Train LSTM Model
lstm_model = Sequential([
    Input(shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), name="input"),
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(1)
])
lstm_model.compile(optimizer='adam', loss='mse', metrics=[MeanAbsoluteError(), MeanSquaredError()])

history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test_lstm), verbose=2)


# Predict and Inverse Transform
y_pred = lstm_model.predict(X_test_lstm)
y_test_inv = scaler.inverse_transform(np.concatenate((X_test_lstm[:, -1, :], y_test_lstm.reshape(-1, 1)), axis=1))[:, -1]
y_pred_inv = scaler.inverse_transform(np.concatenate((X_test_lstm[:, -1, :], y_pred), axis=1))[:, -1]


# Evaluate Model
mse = mean_squared_error(y_test_inv, y_pred_inv)
mae = mean_absolute_error(y_test_inv, y_pred_inv)
print(f"Test MSE: {mse:.4f}")
print(f"Test MAE: {mae:.4f}")


# Custom Metrics
def calculate_diff_mape(y_true, y_pred):
    abs_diff = np.abs(y_true - y_pred)
    abs_sum = np.abs(y_true) + np.abs(y_pred)
    safe_division = np.where(abs_sum != 0, abs_diff / abs_sum, 0)
    return np.mean(safe_division) * 200


def calculate_diff_rmse(y_true, y_pred):
    return np.sqrt(np.mean((y_true - y_pred)**2))


diff_mape_score = calculate_diff_mape(y_test_inv, y_pred_inv)
diff_rmse_score = calculate_diff_rmse(y_test_inv, y_pred_inv)
print(f"DiffMAPE Score: {diff_mape_score:.4f}")
print(f"DiffRMSE: {diff_rmse_score:.4f}")


# Visualization of Predictions (Figures 15-18)
plt.figure(figsize=(12, 6))
plt.plot(y_test_inv, label='Actual', alpha=0.7)
plt.plot(y_pred_inv, label='Predicted', alpha=0.7)
plt.title('Actual vs Predicted SAN Load')
plt.xlabel('Sample Index')
plt.ylabel('SAN Load')
plt.legend()
plt.show()


# Training and Validation Loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


# Save Results
catboost_model.save_model('catboost_model.cbm')
lstm_model.save('lstm_model.h5')
predictions = pd.DataFrame({'Actual': y_test_inv, 'Predicted': y_pred_inv})
predictions.to_csv('predictions.csv', index=False)


print("Updated visualization and model outputs generated, including all relevant figures.")

